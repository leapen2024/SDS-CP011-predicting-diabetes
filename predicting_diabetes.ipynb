{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leapen2024/SDS-CP011-predicting-diabetes/blob/main/predicting_diabetes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akD24gZp_OYi"
      },
      "source": [
        "# **Project Title**\n",
        "# Predicting Diabetes Likelihood and Classifying Diabetes Types Using Demographic and Clinical Data: A Hybrid Machine Learning Approach\n",
        "\n",
        "### **0. Background & Motivation**\n",
        "* Diabetes is a chronic, progressive condition that affects millions worldwide, leading to severe health complications such as cardiovascular disease, kidney failure, neuropathy, and vision problems. The burden of diabetes is growing globally, with pre-diabetes and undiagnosed cases rising. Early detection and intervention are critical to reducing diabetes-related complications and healthcare costs.\n",
        "\n",
        "* This project leverages machine learning to predict the likelihood of diabetes and classify individuals into relevant categories (e.g., pre-diabetes, Type 1, Type 2).\n",
        "* A flexible, data-driven approach will help healthcare professionals identify high-risk individuals and tailor interventions effectively.\n",
        "* The goal is to provide both probabilistic risk scores and binary classifications, allowing clinicians to make more informed decisions.\n",
        "\n",
        "### **1. Objective**\n",
        "* The primary objective is to develop a machine learning model that predicts the likelihood of diabetes and classifies individuals into diabetes categories based on demographic and clinical data. The secondary objective is to ensure the model's interpretability for clinical use to improve patient outcomes.\n",
        "\n",
        "### **Project Phases**\n",
        "#### **Phase 1: Data Cleaning & Analysis (Week 1)**\n",
        "* Tasks:\n",
        "\n",
        "* Review and clean the dataset (handle missing values, inconsistencies, outliers).\n",
        "Perform exploratory data analysis (EDA) to identify trends, correlations, and key factors influencing diabetes.\n",
        "Univariate, bivariate, and multivariate analysis to explore the relationships between features and the diabetes outcome.\n",
        "\n",
        "###### **Milestone:**\n",
        "\n",
        "***Cleaned dataset and comprehensive EDA report, including feature insights and correlations.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHdxvRno-rLs"
      },
      "source": [
        "# Upload Data to Google Colab\n",
        "**1: Upload the Dataset from Local**\n",
        "* To upload a dataset directly from your local machine, follow these steps:\n",
        "   * Open Google Colab in your browser.\n",
        "   * Start a new notebook.\n",
        "   * Use the following code to upload the CSV file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "RutLeH8vOA06",
        "outputId": "d908a60f-7834-4669-d240-0b77c35f7d96"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7e81f6d0-6c98-4cb0-996a-ebc4df7703b0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7e81f6d0-6c98-4cb0-996a-ebc4df7703b0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVwifOn-CGLc"
      },
      "outputs": [],
      "source": [
        "# System and OS libraries\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Ignore warnings for clean output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Data analysis libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine learning libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Set visualization settings\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "# Colab specific imports (for file uploads)\n",
        "# from google.colab import files\n",
        "\n",
        "# Now we can proceed to load the dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GYe2LXWCSLw"
      },
      "source": [
        "## Explanation of Libraries:\n",
        "#### OS & Warnings:\n",
        "\n",
        "* os: For interacting with the file system (if needed).\n",
        "* warnings: To suppress unnecessary warnings for cleaner output.\n",
        "*Data Analysis:\n",
        "  * pandas: For data manipulation and analysis.\n",
        "  * numpy: For numerical operations and handling arrays.\n",
        "*Data Visualization:\n",
        "  * matplotlib.pyplot: For creating static visualizations.\n",
        "  * seaborn: For making statistical data visualizations (based on matplotlib).\n",
        "*Machine Learning:\n",
        "  * sklearn.model_selection: For splitting the data and hyperparameter tuning.\n",
        "  * sklearn.preprocessing: For scaling and encoding the data.\n",
        "  * sklearn.ensemble: Random forest classifier for initial model testing.\n",
        "  * sklearn.linear_model: Logistic regression for likelihood prediction.\n",
        "  * sklearn.metrics: To evaluate the model’s performance using accuracy, precision, recall, F1-score, and ROC-AUC.\n",
        "*Colab Specific:\n",
        "  * google.colab.files: To upload files from your local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVID2pW-BuG0"
      },
      "outputs": [],
      "source": [
        "# Load the dataset into a pandas DataFrame\n",
        "# Replace 'diabetes_dataset.csv' with the actual name of your file after uploading\n",
        "df = pd.read_csv('diabetes_dataset - diabetes_dataset.csv')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2cqrb4LKLwF"
      },
      "source": [
        "**2. Inspect the Data**\n",
        "* Once the dataset is loaded, it’s time to inspect it to understand its structure, identify any potential issues, and check for missing values or inconsistencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u85ESjdyQeQn"
      },
      "outputs": [],
      "source": [
        "# Check the shape of the data\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVCXtHPIK0Wd"
      },
      "outputs": [],
      "source": [
        "# Check data types and basic structure\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiKUWdDjK5qO"
      },
      "outputs": [],
      "source": [
        "# Get summary statistics of numerical columns\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqWVf1_zL_Ed"
      },
      "source": [
        "**This output provides summary statistics for the dataset, allowing us to understand the distribution of each feature.**\n",
        "1. General Information\n",
        "  - Number of observations:\n",
        "    - Each column has 100,000 entries (count = 100000), so the dataset is complete with no missing values.\n",
        "2. Year\n",
        "  - Mean year: The average year is 2018.36, meaning most data was collected around 2018-2019.\n",
        "  - Range: The data spans from 2015 to 2022, which provides a recent and relevant dataset for predicting current diabetes trends.\n",
        "3. Age\n",
        "  - Mean age: The average age is 41.88 years, indicating the dataset includes a broad adult population.\n",
        "  - Range: The minimum age is 0.08 years (likely representing infants), while the maximum age is 80 years.\n",
        "  - Distribution: The quartiles (25%, 50%, 75%) show that 50% of the population is between 24 and 60 years old. The median age is 43.\n",
        "4. Race Representation\n",
        "  - The dataset has binary indicators (0 or 1) for each race category:\n",
        "    - race columns are used to represent racial identity, with the value \"1\" indicating belonging to that race.\n",
        "      - African American: 20.2% of the dataset (mean = 0.20223).\n",
        "Asian: 20% (mean = 0.20015).\n",
        "Caucasian: 19.8% (mean = 0.19876).\n",
        "Hispanic: 19.9% (mean = 0.19888).\n",
        "Other: 20% (mean = 0.19998).\n",
        "This suggests a fairly balanced dataset across racial categories.\n",
        "\n",
        "5. Hypertension\n",
        "Mean: 7.48% of individuals have hypertension (mean = 0.07485).\n",
        "Range: Values are binary (0 or 1), indicating whether a person has hypertension.\n",
        "6. Heart Disease\n",
        "Mean: 3.94% of individuals have heart disease (mean = 0.03942).\n",
        "Range: This is also a binary variable (0 or 1).\n",
        "7. BMI (Body Mass Index)\n",
        "Mean BMI: The average BMI is 27.32, which falls in the \"overweight\" category (25–29.9).\n",
        "Range: BMI ranges from 10.01 (underweight) to 95.69 (extremely high, suggesting a rare or incorrect entry).\n",
        "Distribution: The majority of individuals have a BMI between 23.63 and 29.58, as indicated by the 25th and 75th percentiles.\n",
        "8. HbA1c Level\n",
        "Mean HbA1c: The average HbA1c level is 5.53%, which is close to the upper limit of normal (5.7%).\n",
        "Range: Values range from 3.5% to 9%, with higher values indicating diabetes.\n",
        "Distribution: The interquartile range (IQR) shows that 50% of the population has HbA1c levels between 4.8% and 6.2%. The median is 5.8%, indicating that some individuals are already in the pre-diabetes or diabetes range.\n",
        "9. Blood Glucose Level\n",
        "Mean blood glucose: The average blood glucose level is 138.06 mg/dL, which is above normal (typically 70-100 mg/dL), indicating that many individuals in the dataset may have diabetes or are at risk.\n",
        "Range: Blood glucose levels range from 80 mg/dL to 300 mg/dL, with extreme values suggesting severe hyperglycemia or outliers.\n",
        "Distribution: The median is 140 mg/dL, and the 75th percentile is 159 mg/dL, indicating that a significant portion of the dataset could be in the pre-diabetic or diabetic range.\n",
        "10. Diabetes Status (Target Variable)\n",
        "Mean: The mean value is 0.085, indicating that approximately 8.5% of individuals have been diagnosed with diabetes.\n",
        "Range: This is a binary variable (0 or 1), with \"1\" representing individuals diagnosed with diabetes.\n",
        "Key Insights:\n",
        "Age: The dataset has a wide age range, with a median age of 43, which aligns well with a target population for diabetes risk assessment.\n",
        "Race Representation: The dataset includes diverse racial groups, with each category representing approximately 20% of the population.\n",
        "Health Conditions:\n",
        "Hypertension (7.48%) and heart disease (3.94%) are present in a small subset of the population, but these comorbidities are critical for diabetes risk modeling.\n",
        "BMI & HbA1c: Many individuals in the dataset are overweight or have elevated HbA1c, which suggests a high prevalence of pre-diabetes or diabetes.\n",
        "Blood Glucose: A high average glucose level, along with elevated HbA1c levels, reinforces the likelihood of diabetes within this population.\n",
        "Diabetes Diagnosis: 8.5% of the population has diabetes, providing a reasonable target size for the classification model.\n",
        "These insights help us better understand the dataset, enabling us to focus on the most critical features (age, BMI, HbA1c, blood glucose, and comorbidities) for our diabetes risk prediction and classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agy1ZxtfLiZF"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsXxTZAXLnaY"
      },
      "outputs": [],
      "source": [
        "# Check for duplicate rows in the dataset\n",
        "duplicates = df.duplicated().sum()\n",
        "\n",
        "# Print the number of duplicate rows\n",
        "print(f\"Number of duplicate rows: {duplicates}\")\n",
        "\n",
        "# If there are duplicates, you can drop them\n",
        "if duplicates > 0:\n",
        "    df = df.drop_duplicates()\n",
        "    print(f\"Duplicates removed. New dataset size: {df.shape}\")\n",
        "else:\n",
        "    print(\"No duplicates found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0WxVdwkWVlH"
      },
      "source": [
        "* Visualize the Distribution of Key Features\n",
        "    * We’ll start by visualizing the distributions of important continuous features such as age, BMI, HbA1c levels, and blood glucose levels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESabwzJdVcvu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set up the visualizations\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "# Histogram of Age\n",
        "plt.subplot(2, 2, 1)\n",
        "sns.histplot(df['age'], kde=True, bins=30)\n",
        "plt.title('Age Distribution')\n",
        "\n",
        "# Histogram of BMI\n",
        "plt.subplot(2, 2, 2)\n",
        "sns.histplot(df['bmi'], kde=True, bins=30)\n",
        "plt.title('BMI Distribution')\n",
        "\n",
        "# Histogram of HbA1c Level\n",
        "plt.subplot(2, 2, 3)\n",
        "sns.histplot(df['hbA1c_level'], kde=True, bins=30)\n",
        "plt.title('HbA1c Level Distribution')\n",
        "\n",
        "# Histogram of Blood Glucose Level\n",
        "plt.subplot(2, 2, 4)\n",
        "sns.histplot(df['blood_glucose_level'], kde=True, bins=30)\n",
        "plt.title('Blood Glucose Level Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXoLiZCxXJnx"
      },
      "source": [
        "* Correlation Analysis\n",
        "    * We’ll explore how the features are correlated with one another, especially focusing on health-related features like BMI, HbA1c, and blood glucose levels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8AideIcWiT9"
      },
      "outputs": [],
      "source": [
        "# Select only numeric columns from the DataFrame\n",
        "numeric_cols = df.select_dtypes(include=[np.number])\n",
        "\n",
        "# Check the selected numeric columns\n",
        "print(numeric_cols.columns)\n",
        "\n",
        "# Now, calculate the correlation matrix with the numeric columns\n",
        "correlation_matrix = numeric_cols.corr()\n",
        "\n",
        "# Plot the correlation matrix\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Matrix (Numeric Features Only)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SxuE-GkY35-"
      },
      "source": [
        "Relationships Between Demographics and Health Metrics\n",
        "We can visualize how demographic factors (e.g., age, race) influence key health metrics and the likelihood of having diabetes.\n",
        "\n",
        "Boxplots to Explore Demographic Groups\n",
        "This will help identify differences in BMI, HbA1c, and blood glucose levels across age groups, gender, and race."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZoP2NN2XPo0"
      },
      "outputs": [],
      "source": [
        " # Age vs. HbA1c Level\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='age', y='hbA1c_level', data=df)\n",
        "plt.title('HbA1c Levels Across Age')\n",
        "\n",
        "# Race vs. HbA1c Level\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='race:AfricanAmerican', y='hbA1c_level', data=df)  # You can switch between race groups\n",
        "plt.title('HbA1c Levels Across AfricanAmerican Race')\n",
        "\n",
        "# Race vs. HbA1c Level\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='race:Asian', y='hbA1c_level', data=df)  # You can switch between race groups\n",
        "plt.title('HbA1c Levels Across Asian Race')\n",
        "\n",
        "# Race vs. HbA1c Level\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='race:Caucasian', y='hbA1c_level', data=df)  # You can switch between race groups\n",
        "plt.title('HbA1c Levels Across Caucasian Race')\n",
        "\n",
        "# Race vs. HbA1c Level\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='race:Hispanic', y='hbA1c_level', data=df)  # You can switch between race groups\n",
        "plt.title('HbA1c Levels Across Hispanic Race')\n",
        "\n",
        "# Race vs. HbA1c Level\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='race:Other', y='hbA1c_level', data=df)  # You can switch between race groups\n",
        "plt.title('HbA1c Levels Across Other Races')\n",
        "\n",
        "# Gender vs. BMI\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='gender', y='bmi', data=df)\n",
        "plt.title('BMI Distribution Across Genders')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaiZ87LWasHP"
      },
      "source": [
        "Analysis of Target Variable (Diabetes Status)\n",
        "Let’s also look at the distribution of the binary target variable to understand how many individuals are classified as having diabetes versus not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig30zmJtY7IU"
      },
      "outputs": [],
      "source": [
        "# Check the distribution of the target variable\n",
        "df['diabetes'].value_counts()\n",
        "\n",
        "# Plot the distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='diabetes', data=df)\n",
        "plt.title('Distribution of Diabetes Status (0 = No, 1 = Yes)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw7TT-pDyKH-"
      },
      "source": [
        "The distribution of diabetes status in the dataset shows a significant class imbalance. Most of the individuals in the dataset fall into the non-diabetic (0) category, with a much smaller number in the diabetic (1) category. This type of class imbalance can cause challenges in training a model, particularly for the minority class (diabetic individuals).\n",
        "\n",
        "Challenges of Class Imbalance:\n",
        "Bias in Prediction: Models may become biased toward predicting the majority class (non-diabetic) and fail to effectively predict the minority class (diabetic), leading to poor performance on metrics like recall and precision for the diabetic class.\n",
        "Evaluation Metrics: Standard accuracy may not be a reliable metric in this case, as a model could achieve high accuracy by simply predicting most individuals as non-diabetic (due to the class imbalance).\n",
        "Techniques to Handle Class Imbalance:\n",
        "1. Resampling the Dataset:\n",
        "Oversampling the Minority Class: Increase the number of diabetic samples by duplicating existing diabetic cases or generating synthetic data (e.g., using SMOTE).\n",
        "Undersampling the Majority Class: Reduce the number of non-diabetic samples to balance the classes, though this can result in loss of important data.\n",
        "Example of using SMOTE (Synthetic Minority Over-sampling Technique)\n",
        "\n",
        "2. Using Class Weighting in Models:\n",
        "Most machine learning algorithms allow you to set class weights to penalize the model more for misclassifying the minority class. This helps the model focus more on learning patterns in the diabetic group.\n",
        "For example, in Random Forest\n",
        "\n",
        "3. Evaluation Metrics for Imbalanced Data:\n",
        "Use metrics like F1-score, Precision-Recall AUC, and ROC-AUC to better evaluate how well the model is performing, especially on the minority class (diabetes)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bkKTMTWy2yQ"
      },
      "source": [
        "##############################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FlyJ0vcy6Tz"
      },
      "source": [
        "- Whether you should handle the class imbalance before or after modeling depends on the approach you want to take.\n",
        "\n",
        "- Option 1: Handle Class Imbalance Before Modeling (Preprocessing Stage)\n",
        "When to Choose: If you want to balance the dataset itself before feeding it into the model, you can use techniques like SMOTE (oversampling) or undersampling in the preprocessing step.\n",
        "Benefits: This allows the model to learn from a balanced dataset right from the beginning. It helps in scenarios where you want a more even representation of each class (diabetic and non-diabetic) in the training process.\n",
        "Pros:\n",
        "Models are exposed to a balanced dataset during training.\n",
        "You avoid overwhelming the model with too many non-diabetic cases, which can bias predictions.\n",
        "Cons:\n",
        "Overfitting: If you oversample the minority class (e.g., using SMOTE), the model might overfit to the generated or duplicated diabetic cases.\n",
        "Data Loss: If you undersample the majority class, you may lose valuable information.\n",
        "- Option 2: Handle Class Imbalance During Modeling\n",
        "When to Choose: Most modern machine learning algorithms, such as Random Forest, XGBoost, and Logistic Regression, allow you to handle class imbalance within the model using the class weight parameter. This option enables the model to put more emphasis on misclassifying the minority class (diabetes).\n",
        "Benefits: You avoid modifying the dataset itself and instead make the model focus more on learning patterns in the minority class.\n",
        "Pros:\n",
        "Avoids data loss or overfitting issues caused by oversampling or undersampling.\n",
        "You retain the original distribution of the dataset.\n",
        "Models can be easily configured to pay more attention to the minority class without changing the data.\n",
        "Cons:\n",
        "Class weighting can be a bit harder to tune for some models, and it might not always yield better performance.\n",
        "When to Use Each Approach:\n",
        "Preprocessing Approach (SMOTE or Undersampling):\n",
        "Best for simpler models where the dataset needs to be more balanced before training.\n",
        "Works well for small datasets or when you want to have balanced data throughout the entire process.\n",
        "Class Weighting Approach (During Model Training):\n",
        "Best for complex models (Random Forest, Gradient Boosting, Logistic Regression) that have built-in support for class weighting.\n",
        "Recommended for larger datasets, where duplicating or removing data can result in performance degradation.\n",
        "Recommendation:\n",
        "Since you're in the early stages of modeling and have a significant class imbalance, I recommend starting with class weighting during the modeling stage, because:\n",
        "\n",
        "It’s non-destructive: You don't alter the original data.\n",
        "It’s easier to implement and tune: You can directly apply the weights to your chosen models without changing the dataset.\n",
        "If class weighting doesn’t work well, you can always go back and apply oversampling or undersampling in the preprocessing step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmgbFyuN0jZf"
      },
      "source": [
        "###########################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1XS-7k_0lmC"
      },
      "source": [
        "What’s the Problem?\n",
        "Class imbalance means we have many more non-diabetic cases (0) than diabetic cases (1).\n",
        "If we don’t address this, a model might predict most people as non-diabetic, because there are many more of them. This can lead to a high accuracy but poor performance when identifying the diabetic patients (which are fewer).\n",
        "What Can We Do?\n",
        "There are two ways to handle class imbalance:\n",
        "\n",
        "Before modeling (Preprocessing): Modify the dataset so that the number of diabetic and non-diabetic cases is more balanced.\n",
        "\n",
        "Oversampling: Add more diabetic cases by duplicating them or generating new ones (e.g., using SMOTE).\n",
        "Undersampling: Remove some of the non-diabetic cases so both classes have similar numbers.\n",
        "Pro: The dataset becomes balanced.\n",
        "Con: Can lead to overfitting or loss of valuable data.\n",
        "\n",
        "During modeling (Model training): Tell the model to pay more attention to the diabetic class without changing the data itself. We do this by using a parameter called class weights.\n",
        "\n",
        "Class weights: The model will give more importance to diabetic cases to make sure it doesn’t ignore them just because there are fewer.\n",
        "Pro: No data is removed or duplicated; we let the model handle the imbalance.\n",
        "Con: Sometimes harder to fine-tune, but it's a safe first step.\n",
        "\n",
        "What Did We Decide?\n",
        "We suggested starting by using class weights in the model because:\n",
        "\n",
        "It's simple and doesn't modify the data.\n",
        "It allows the model to still learn from the original dataset but with a focus on the diabetic cases.\n",
        "If it works well, you avoid the risk of overfitting or data loss that can come from modifying the dataset.\n",
        "What Will Happen When We Use Class Weights?\n",
        "The model will be more sensitive to diabetic cases.\n",
        "It won’t just predict non-diabetic (0) because that’s the majority.\n",
        "It will attempt to balance between non-diabetic and diabetic cases during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFWIOxrC-tJ3"
      },
      "source": [
        "######################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJoSHLYr-xKN"
      },
      "source": [
        "What We Are Doing:\n",
        "We will train a Random Forest model (which supports class weights) on the original dataset.\n",
        "We'll use class weights to make the model more sensitive to predicting diabetic cases, ensuring that it doesn't just predict the majority class (non-diabetic).\n",
        "Step-by-Step Process:\n",
        "Define Features and Target:\n",
        "\n",
        "The features (X) are all the columns except for the target variable (diabetes).\n",
        "The target (y) is the diabetes column, where 0 means no diabetes and 1 means diabetes.\n",
        "Split the Data:\n",
        "\n",
        "We'll split the data into a training set (to train the model) and a test set (to evaluate the model’s performance).\n",
        "Train the Random Forest Model with Class Weights:\n",
        "\n",
        "The class_weight='balanced' option in Random Forest will handle the class imbalance by automatically adjusting the weights for each class based on the class distribution.\n",
        "Evaluate the Model:\n",
        "\n",
        "After training the model, we’ll evaluate it using metrics like precision, recall, F1-score, and ROC-AUC to see how well it predicts both diabetic and non-diabetic cases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBynfF52yL9C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 1: One-hot encode the categorical variables\n",
        "df_encoded = pd.get_dummies(df, drop_first=True)  # This encodes categorical columns into numerical format\n",
        "\n",
        "# Step 2: Define the feature matrix (X) and the target vector (y)\n",
        "X = df_encoded.drop('diabetes', axis=1)  # All columns except the target\n",
        "y = df_encoded['diabetes']  # The target column (0 = no diabetes, 1 = diabetes)\n",
        "\n",
        "# Step 3: Split the data into training and testing sets (70% training, 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Initialize RandomForestClassifier with class weights\n",
        "rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
        "\n",
        "# Step 5: Train the model on the training data\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions on the test data\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluate the model performance\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t55MLLiDLQO"
      },
      "source": [
        "Steps to Visualize the Encoded Dataset:\n",
        "Apply One-Hot Encoding.\n",
        "Display a Sample of the Encoded Data using head() to see the transformed columns.\n",
        "Optionally, you can use a heatmap to visualize correlations between the new features after encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BVXydYPDPR-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Apply one-hot encoding\n",
        "df_encoded = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Step 2: Display the first few rows of the encoded dataset to see the changes\n",
        "print(\"Encoded Dataset Sample:\")\n",
        "print(df_encoded.head())\n",
        "\n",
        "# Step 3: Visualize correlations using a heatmap (optional)\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(df_encoded.corr(), annot=False, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap of Encoded Dataset')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfx7a0_3arWb"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYNgYertbAoV"
      },
      "outputs": [],
      "source": [
        "# Get the list of column headers in the DataFrame\n",
        "column_headers = df.columns\n",
        "\n",
        "# Print the column headers\n",
        "print(column_headers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN5iB_AeeFJW"
      },
      "source": [
        "Automated Box Plots for Categorical vs Numeric Variables\n",
        "You can loop through all the categorical variables and plot them against all the numeric variables in your dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elKKHF-ubIUH"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Separate categorical and numeric columns\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Plot each categorical variable against each numeric variable\n",
        "for cat_col in categorical_cols:\n",
        "    for num_col in numeric_cols:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.boxplot(x=cat_col, y=num_col, data=df)\n",
        "        plt.title(f'{num_col} Distribution by {cat_col}')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZyZv-x0hdId"
      },
      "source": [
        "Scatter Plots for Numeric vs Numeric Variables\n",
        "For numeric-numeric relationships, you can use scatter plots to visualize the relationships between different numeric features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwEiGjY-eH4s"
      },
      "outputs": [],
      "source": [
        "  # Plot each numeric variable against each other numeric variable\n",
        "for num_col1 in numeric_cols:\n",
        "    for num_col2 in numeric_cols:\n",
        "        if num_col1 != num_col2:  # Avoid plotting the same variable against itself\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            sns.scatterplot(x=num_col1, y=num_col2, data=df)\n",
        "            plt.title(f'{num_col1} vs {num_col2}')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDa4tnB3iJ42"
      },
      "source": [
        "* Pairwise Plot for Numeric Variables\n",
        "    * If you'd like a comprehensive visualization of relationships between all numeric variables, you can use pairplot to create a grid of scatter plots for each pair of numeric features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69ZxVE5LhifT"
      },
      "outputs": [],
      "source": [
        "# Pairwise plot for all numeric variables\n",
        "sns.pairplot(df[numeric_cols])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqDejylKDqSF"
      },
      "source": [
        "Checking for multicollinearity is an important step, especially when working with a large number of features, including one-hot encoded categorical variables. Multicollinearity occurs when two or more features are highly correlated, which can cause problems in some models (like linear regression), making the model coefficients unstable and difficult to interpret.\n",
        "\n",
        "Why Multicollinearity Matters:\n",
        "Redundancy: If two or more features are highly correlated, they might be carrying the same information. This can reduce model interpretability.\n",
        "Instability: In models like linear regression, multicollinearity can make the model coefficients fluctuate dramatically with small changes in the data.\n",
        "Not an issue for all models: Some models, such as tree-based models (like Random Forest), are not very sensitive to multicollinearity because they make splits based on the most useful feature at each step. However, it’s still a good practice to detect and handle it.\n",
        "How to Detect Multicollinearity:\n",
        "Variance Inflation Factor (VIF): VIF quantifies how much the variance of a regression coefficient is inflated due to multicollinearity. A VIF above 5-10 typically indicates high multicollinearity.\n",
        "\n",
        "Correlation Matrix: We can check for any features that have high correlation (close to 1 or -1) with each other. However, this can miss more complex cases of multicollinearity that VIF would detect.\n",
        "\n",
        "1. Using VIF to Detect Multicollinearity:\n",
        "Let’s compute the Variance Inflation Factor (VIF) for the features in your dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srt253PvDp82"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Step 1: Apply one-hot encoding (if not already done)\n",
        "df_encoded = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Step 2: Ensure all data is numeric (convert any bool columns to int)\n",
        "df_encoded = df_encoded.apply(lambda col: col.astype(int) if col.dtypes == 'bool' else col)\n",
        "\n",
        "# Step 3: Define the feature matrix (X) without the target variable 'diabetes'\n",
        "X = df_encoded.drop('diabetes', axis=1)\n",
        "\n",
        "# Step 4: Calculate VIF for each feature\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "# Step 5: Display VIF scores\n",
        "print(vif_data.sort_values(by=\"VIF\", ascending=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHsHyMB1GfZ5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy import stats\n",
        "\n",
        "# -------- Phase 2: Data Preprocessing & Feature Engineering -------- #\n",
        "\n",
        "# 1. Handle Missing Data (Imputation)\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df['bmi'] = imputer.fit_transform(df[['bmi']])\n",
        "\n",
        "# 2. Create New Features (Interaction Terms)\n",
        "df['age_bmi_interaction'] = df['age'] * df['bmi']\n",
        "\n",
        "# Print column names to verify\n",
        "print(\"Columns in DataFrame:\", df.columns)\n",
        "\n",
        "# Check if the required columns exist before encoding\n",
        "columns_to_encode = ['gender', 'location', 'smoking_history']\n",
        "for col in columns_to_encode:\n",
        "    if col not in df.columns:\n",
        "        print(f\"Column '{col}' not found in dataframe.\")\n",
        "\n",
        "# 3. One-Hot Encoding for Categorical Variables (Gender, Location, Smoking History)\n",
        "encoder = OneHotEncoder(drop='first', sparse_output=False)  # Updated 'sparse_output' argument\n",
        "\n",
        "# Ensure the columns are present before encoding\n",
        "if all(col in df.columns for col in columns_to_encode):\n",
        "    encoded_gender_location_smoking = encoder.fit_transform(df[columns_to_encode])\n",
        "    encoded_df = pd.DataFrame(encoded_gender_location_smoking, columns=encoder.get_feature_names_out(columns_to_encode))\n",
        "\n",
        "    # Concatenate encoded variables with the original dataframe\n",
        "    df = pd.concat([df, encoded_df], axis=1)\n",
        "\n",
        "    # Drop the original categorical columns only if they exist\n",
        "    df = df.drop(columns=[col for col in columns_to_encode if col in df.columns])\n",
        "\n",
        "else:\n",
        "    print(\"Some columns are missing; cannot perform encoding.\")\n",
        "\n",
        "\n",
        "# 4. Scale Continuous Variables (bmi, hbA1c_level, blood_glucose_level)\n",
        "scaler = StandardScaler()\n",
        "df[['bmi', 'hbA1c_level', 'blood_glucose_level']] = scaler.fit_transform(df[['bmi', 'hbA1c_level', 'blood_glucose_level']])\n",
        "\n",
        "# 5. Handle Outliers in BMI and Blood Glucose Levels using Z-score\n",
        "z_scores = stats.zscore(df[['bmi', 'blood_glucose_level']])\n",
        "df_no_outliers = df[(abs(z_scores) < 3).all(axis=1)]\n",
        "\n",
        "# Displaying the final cleaned and preprocessed data\n",
        "df_no_outliers.to_csv('cleaned_preprocessed_data.csv', index=False)\n",
        "\n",
        "print(\"Data Preprocessing Completed. Cleaned data saved as 'cleaned_preprocessed_data.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTXXIgo2OkFI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.getcwd())  # This will print the current working directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29LN8TAFPBbh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.getcwd())  # This will print /content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAoMJMtiPYjn"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('/content/cleaned_preprocessed_data.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_Ljtc3QRJJ3"
      },
      "outputs": [],
      "source": [
        "print(df.columns)  # This will show all column names in the dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQ4z5MBORfrw"
      },
      "outputs": [],
      "source": [
        "# Group Locations into Broader Regions:\n",
        "\n",
        "# Define groups of states for each region\n",
        "east_states = ['location_New York', 'location_Massachusetts', 'location_New Jersey', 'location_Pennsylvania']\n",
        "west_states = ['location_California', 'location_Washington', 'location_Oregon']\n",
        "south_states = ['location_Texas', 'location_Florida', 'location_Georgia']\n",
        "north_states = ['location_Illinois', 'location_Michigan', 'location_Wisconsin']\n",
        "\n",
        "# Create a new column for each region by summing the corresponding one-hot encoded columns\n",
        "df['region_east'] = df[east_states].sum(axis=1)\n",
        "df['region_west'] = df[west_states].sum(axis=1)\n",
        "df['region_south'] = df[south_states].sum(axis=1)\n",
        "df['region_north'] = df[north_states].sum(axis=1)\n",
        "\n",
        "# Optionally drop the individual one-hot encoded location columns\n",
        "df = df.drop(columns=east_states + west_states + south_states + north_states)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_rEjRRVRrov"
      },
      "outputs": [],
      "source": [
        "# Frequency Encoding for Locations:\n",
        "\n",
        "# Get the frequency of each one-hot encoded location column (sum of 1s in each column)\n",
        "location_columns = [col for col in df.columns if col.startswith('location_')]\n",
        "freq_encoded = {col: df[col].sum() for col in location_columns}\n",
        "\n",
        "# Create a new column 'location_encoded' that applies the frequency to each row\n",
        "df['location_encoded'] = df[location_columns].dot(pd.Series(freq_encoded))\n",
        "\n",
        "# Optionally drop the individual one-hot encoded location columns\n",
        "df = df.drop(columns=location_columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-YIFfKSR65G"
      },
      "outputs": [],
      "source": [
        "print(df['location_encoded'].head())  # Check a few rows of the new column\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i33PwmaNSDxn"
      },
      "outputs": [],
      "source": [
        "print(df.isnull().sum())  # Check for any missing values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGkrBJoYSg2_"
      },
      "source": [
        "#### There are no missing values. So, let's proceed.\n",
        "\n",
        "* Since there are no missing values, and you have already encoded your location data into broader regions (region_east, region_west, region_south, region_north) and applied frequency encoding (location_encoded), you can now finalize the preprocessed dataset.\n",
        "\n",
        "* Check Data Distributions: Visualize the key features in your dataset to ensure everything looks correct and consistent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVqOtZvbSgUH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "df.hist(figsize=(10, 8))\n",
        "plt.show()  # Check distributions of all numeric features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC-7N6ITT5SQ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot histograms with adjusted figure size and layout\n",
        "df.hist(figsize=(15, 12), bins=20)\n",
        "plt.tight_layout()  # Adjusts the layout so labels and plots don't overlap\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAQxD6GjUCTl"
      },
      "outputs": [],
      "source": [
        "# Plot Specific Columns:\n",
        "# continuous variables like age, bmi, hbA1c_level), you can plot only those columns to reduce clutter.\n",
        "\n",
        "# Plot specific continuous variables\n",
        "df[['age', 'bmi', 'hbA1c_level', 'blood_glucose_level']].hist(figsize=(10, 8), bins=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_GdxeFvUpCK"
      },
      "outputs": [],
      "source": [
        "# Rotate X-Axis Labels:\n",
        "# Rotate x-axis labels\n",
        "df.hist(figsize=(15, 12), bins=20)\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJZo3RSWUyFm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zvq9JzFS4lA"
      },
      "source": [
        "### Feature Scaling:\n",
        "* Before we move on to model training, let's ensure that the continuous variables are scaled. We have scaled bmi, hbA1c_level, and blood_glucose_level, but we may also want to scale new features like location_encoded and any others that were added."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkKp-RG2TPyT"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# List of continuous variables to scale\n",
        "features_to_scale = ['bmi', 'hbA1c_level', 'blood_glucose_level', 'location_encoded',\n",
        "                     'age_bmi_interaction']\n",
        "\n",
        "# Apply scaling\n",
        "df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n",
        "\n",
        "# Check the scaled data\n",
        "print(df[features_to_scale].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy2JvvS1TZhG"
      },
      "source": [
        "### Train-Test Split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKt_PoZPTkbn"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df.drop(columns=['diabetes'])  # Features, excluding the target\n",
        "y = df['diabetes']  # Target\n",
        "\n",
        "# Split into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the sizes of training and testing sets\n",
        "print(f\"Training set size: {X_train.shape}\")\n",
        "print(f\"Test set size: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81wUlsC0U3CP"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets (80% training, 20% testing)\n",
        "X = df.drop(columns=['diabetes'])  # Features, excluding the target\n",
        "y = df['diabetes']  # Target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Model accuracy\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(f\"Logistic Regression Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cma40wEeU9Pw"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Train the Random Forest model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Model accuracy\n",
        "rf_accuracy = rf_model.score(X_test, y_test)\n",
        "print(f\"Random Forest Accuracy: {rf_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPZseCqOVgSB"
      },
      "source": [
        "### Model Evaluation: using classification metrics such as accuracy, precision, recall, F1-score, and confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGdgKAEBVu0i"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Get predictions\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion matrix\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YglcdMaIXbQN"
      },
      "source": [
        "### Performance Insights:\n",
        "* Accuracy:\n",
        "  * The model is correctly predicting the outcome in 97% of the cases, which is quite high.\n",
        "* Precision:\n",
        "  * Precision for class 1 (diabetes) is 0.99, indicating that when the model predicts someone has diabetes, it's correct 99% of the time.\n",
        "* Recall:\n",
        "  * The recall for class 1 is 0.68, meaning the model is able to identify 68% of the actual diabetic cases. This indicates that while the precision is very high, the model may be missing some diabetic cases (false negatives).\n",
        "* F1-Score:\n",
        "  * The F1-score for class 1 is 0.80, showing a decent balance between precision and recall for identifying diabetes. However, there’s room to improve recall, especially for class 1.\n",
        "* Confusion Matrix:\n",
        "  * True Positives (TP): 1,151 diabetic cases were correctly identified.\n",
        "  * False Negatives (FN): 552 diabetic cases were missed.\n",
        "  * True Negatives (TN): 18,281 non-diabetic cases were correctly identified.\n",
        "  * False Positives (FP): Only 16 cases were wrongly identified as diabetic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gbWfp9gYDIh"
      },
      "source": [
        "### mprove Recall for Class 1: The recall for detecting diabetic cases (class 1) could be improved with the following:\n",
        "\n",
        "* Class Imbalance Handling: It looks like there is a class imbalance (with far more non-diabetic cases than diabetic cases). You can try upsampling or downsampling or use techniques like SMOTE (Synthetic Minority Over-sampling Technique).\n",
        "* Adjust Class Weights: In the RandomForestClassifier, you can adjust the class_weight parameter to put more emphasis on the minority class (1). This can help the model focus on detecting diabetic cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j21FOG27YC3C"
      },
      "outputs": [],
      "source": [
        "rf_model = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
        "rf_model.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HQ2mhquZspZ"
      },
      "source": [
        "### Experimenting with Other Models\n",
        "* Gradient Boosting: We'll train a GradientBoostingClassifier and check the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5WNeQFzZ4m2"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Train Gradient Boosting model\n",
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and evaluation\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "\n",
        "# Print classification report and confusion matrix\n",
        "print(\"Gradient Boosting Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_gb))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_gb))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c-uxh8wiI23"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkuwkyFjZ_ON"
      },
      "source": [
        "### XGBoost:\n",
        "* Similarly, we'll train an XGBoost model and evaluate its performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lc3uSyUYaBNr"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Train XGBoost model\n",
        "xgb_model = XGBClassifier(random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and evaluation\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# Print classification report and confusion matrix\n",
        "print(\"XGBoost Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_xgb))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK-OJDe0asXj"
      },
      "source": [
        "### Hyperparameter Tuning with GridSearchCV for Random Forest:\n",
        "* Next, let's apply GridSearchCV for hyperparameter tuning of the Random Forest model. We’ll search over parameters like n_estimators, max_depth, and min_samples_split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKQYohi_au8u"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 150],  # Fewer values for faster training\n",
        "    'max_depth': [None, 10],     # Keep fewer options for depth\n",
        "    'min_samples_split': [2, 5]  # Minimal number of splits\n",
        "}\n",
        "\n",
        "# Initialize the model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit GridSearchCV to find the best parameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and model\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "# Predictions and evaluation\n",
        "y_pred_rf = best_rf_model.predict(X_test)\n",
        "\n",
        "# Print classification report and confusion matrix\n",
        "print(\"Tuned Random Forest Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_rf))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_u-lvZQfGKh"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Save the model to a file (for example, Random Forest model)\n",
        "with open('best_rf_model.pkl', 'wb') as file:\n",
        "    pickle.dump(best_rf_model, file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evphTW9ufHEb"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the pickle file\n",
        "files.download('best_rf_model.pkl')\n",
        "\n",
        "# Or, if using joblib\n",
        "# files.download('best_rf_model.joblib')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BceK2oKXL_nQ"
      },
      "outputs": [],
      "source": [
        "print(model.feature_names_in_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvqZZwrNfLdk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}